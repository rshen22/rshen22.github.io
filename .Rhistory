FirstTerm = Lambda_0 * cumsum(F_kj[, 1])[i] - cumsum(Y_1)[i]
# print(paste0("First Term", FirstTerm))
SecondTerm = cumsum(Y_1)[i] * log(cumsum(Y_1)[i]/(Lambda_0 * cumsum(F_kj[, 1])[i]))
# print(paste0("Second Term: ", SecondTerm))
Test_Statistic[i] = FirstTerm + SecondTerm
}
# for (i in 1:k) {
#  SecondTerm_1 = cumsum(Y_1)[i]
#  SecondTerm_2 = log(cumsum(Y_1)[i]/(Lambda_0 * cumsum(F_kj[, 1])[i]))
#  print(SecondTerm_1)
#  print(SecondTerm_2)
#}
Alpha = 3.511749
DataFrame = data.frame(Looks = seq(1, k, by = 1), Test_Statistic = Test_Statistic, Alpha = Alpha)
Types = c("Test_Statistic" = "solid", "Alpha" = "dotted")
NewPlot = ggplot(data = DataFrame) + scale_y_continuous(trans = 'log2') +
geom_line(aes(x = Looks, y = Test_Statistic, linetype = "Test_Statistic")) + geom_point(aes(x = Looks, y = Test_Statistic)) + scale_x_continuous(breaks = seq(1, k, by = 1)) +
geom_line(aes(x = Looks, y = Alpha, linetype = "Alpha")) + scale_linetype_manual(name = "Lines", values = Types) + ylab("Value") +
theme(legend.position = c(0.85, 0.5))
pdf('PreviousMethodology.pdf')
print(NewPlot)
dev.off()
exp(0.04564005)
exp(0.1740369)
exp(0.3079352)
exp( 0.4141762)
source("ResearchCodeFinal.R")
### Load in the Data (RUN THIS CHUNK OF CODE)
### Assumptions: Rate Parameter for Y_k0 and Y_k1 Should be Sufficiently Large for Accurate Results
# because S_k is not defined if Y_k[i] = 0 for any i in {1, ... , k}
library(readxl)
library(ggplot2)
library(tidyverse)
Data = read_excel("Anaphylaxis.xlsx")
Data = data.frame(Data)
names(Data)[names(Data) == "For.lookup"] = "For_Lookup"
names(Data)[names(Data) == "Persons.at.risk.N"] = "Persons_at_Risk_N"
names(Data)[names(Data) == "Person.days.at.risk.N"] = "Person-Days_at_Risk_N"
names(Data)[names(Data) == "Incidence.cases.N"] = "Incident_Cases_N"
names(Data)[names(Data) == "Incident.cases.per.100000.patients.AESI.Neg...100.patients.Preg.Outcome."] =
"Incident_Cases_per_100000_Patients[AESI Neg]/100_Patients[Preg_Outcome]"
names(Data)[names(Data) == "Rate.per.100000.person.years.AESI.Neg...days.Preg.Outcome."] =
"Rate_per_100000_Person_Years[AESI Neg]/Days[Preg_Outcome]"
names(Data)[names(Data) == "Rate.95..lower.CL"] =
"Rate_95_Lower_CL"
names(Data)[names(Data) == "Rate.95..upper.CL"] = "Rate_95_Upper_CL"
ReferenceGroup = Data[1391:1402, ]
Non_ReferenceGroup = Data[1404:1415, ]
ReferenceGroup$Persons_at_Risk_N = as.double(ReferenceGroup$Persons_at_Risk_N )
# In the Paper (1nd Example): Reference: Age 2-4 ALL 2017 (497:508), Non-Reference: Age 5-11 ALL 2018 (566:577)
# In the Paper (2nd Example): Reference: Age 5-11 Female 2018 (1391:1402), Non-Reference: Age 5-11 Female 2019 (1404:1415)
# In the Paper (3rd Example): Reference: Age 18-25 2018 (126:137), Non-Reference: Age 18-25 2019 (139:150)
k = nrow(ReferenceGroup)
F_kj = c(ReferenceGroup$`Person-Days_at_Risk_N`, Non_ReferenceGroup$`Person-Days_at_Risk_N`) #
F_kj = matrix(F_kj, nrow = k, ncol = 2)
R_k = F_kj[, 2]/F_kj[, 1]
Y_k = ReferenceGroup$Incident_Cases_N + Non_ReferenceGroup$Incident_Cases_N
LambdaReferenceGroup = as.vector(ReferenceGroup$Incident_Cases_N/F_kj[, 1])
Observed_S = rep(NA, k)
for (i in 1:k) {
Observed_S[i] = cumsum(Non_ReferenceGroup$Incident_Cases_N/Y_k)[i]/cumsum(ReferenceGroup$Incident_Cases_N * R_k/Y_k)[i]
}
### Try Different Alpha Levels ###
Rho = c(-0.5, 0, 0.5)
TotalAlpha = 0.05
S_t = matrix(0, nrow = length(Rho), ncol = k)
for (i in 1:nrow(S_t)) {
Time = seq(1, k, by = 1)
Time_Update = seq(1, k, by = 1)^(Rho[i])
Sum = sum(Time_Update)
Alpha = TotalAlpha * k^(Rho[i]) * 1/Sum
for (j in 1:ncol(S_t)) {
S_t[i, j] = Alpha * (Time[j]/k)^(Rho[i])
}
}
Alpha = S_t
set.seed(2)
TotalData = data.frame(Looks = seq(1, k, by = 1), Observed_S = Observed_S)
for (i in 1:nrow(Alpha)) {
if (Rho[i] < 0) {
eval(parse(text = paste0("Rho_Minus", -Rho[i], " = findCk(Alpha[i, ], Lambda = LambdaReferenceGroup, F_kj, TotalSampled = 10000, Beta_Fixed = 0)")))
eval(parse(text = paste0("TotalData = cbind(TotalData, Rho_Minus", -Rho[i], ")")))
} else {
eval(parse(text = paste0("Rho_", Rho[i], " = findCk(Alpha[i, ], Lambda = LambdaReferenceGroup, F_kj, TotalSampled = 10000, Beta_Fixed = 0)")))
eval(parse(text = paste0("TotalData = cbind(TotalData, Rho_", Rho[i], ")")))
}
}
TotalData = TotalData %>%
gather(Categories, Value, c(Observed_S, Rho_Minus0.5, Rho_0, Rho_0.5))
RejectionGraph = ggplot(data = TotalData, aes(x = Looks, y = Value, color = Categories)) +
geom_line() + geom_point() + scale_x_continuous(breaks = seq(1, k, by = 1)) +
theme(legend.position = c(0.85, 0.80))
RejectionGraph
View(TotalData)
getwd()
setwd("/Users/rexshen/Documents/Github/rshen22.github.io")
knitr::opts_chunk$set(echo = TRUE)
library(censusapi)
library(tidyverse)
library(tigris)
library(sf)
library(leaflet)
Sys.setenv(CENSUS_KEY="c8aa67e4086b4b5ce3a8717f59faa9a28f611dab")
pums_2019_1yr <- getCensus(
name = "acs/acs1/pums",
vintage = 2019,
region = "public use microdata area:*",
regionin = "state:06",
vars = c(
"SERIALNO",
"SPORDER",
"PWGTP",
"WGTP",
"YBL",
"BLD",
"TEN",
"MV",
"HINCP",
"AGEP"
)
)
# saveRDS(pums_2019_1yr, "a6_pums.rds")
# pums_2019_1yr <- readRDS("a6_pums.rds")
ca_pumas <-
pumas("CA", cb = T, progress_bar = F)
sf_boundary <-
counties("CA", cb = T, progress_bar = F) %>%
filter(NAME == "San Francisco")
sf_pumas <-
ca_pumas %>%
st_centroid() %>%
.[sf_boundary, ] %>%
st_drop_geometry() %>%
left_join(ca_pumas %>% select(GEOID10)) %>%
st_as_sf()
View(sf_pumas)
mapview(sf_pumas)
library(mapview)
mapview(sf_pumas)
sf_pums <-
pums_2019_1yr %>%
mutate(
PUMA = str_pad(public_use_microdata_area,5,"left","0")
) %>%
filter(PUMA %in% sf_pumas$PUMACE10)
class(sf_pums$YBL)
sf_pums_clean <- sf_pums %>% filter(YBL %in% c("1", "2", "3"))
sf_pums_clean <- sf_pums %>% mutate(YBL = as.numeric(YBL)) %>%
filter(YBL %in% 1:3) %>% group_by(SERIALNO) %>% summarize_all(first)
View(sf_pums_clean)
sf_pums_clean <- sf_pums %>% mutate(YBL = as.numeric(YBL), AGEP = as.numeric(AGEP)) %>%
filter(YBL %in% 1:3) %>%
arrange(AGEP) %>%
group_by(SERIALNO) %>% summarize_all(first)
View(sf_pums_clean)
Age = ifelse(1 < 2, 5, 6 )
Age
sf_pums_clean <- sf_pums %>% mutate(YBL = as.numeric(YBL), AGEP = as.numeric(AGEP), HINCP = as.numeric(HINCP)) %>%
filter(YBL %in% 1:3) %>%
arrange(AGEP) %>%
group_by(SERIALNO) %>% summarize_all(first) %>% mutate(Leadrisk = ifelse(HINCP < 90000 &
AGEP < 6, 1, 0))
View(sf_pums_clean)
?factor
sf_pums_clean$BLD <- factor(sf_pums_clean$BLD)
class(sf_pums_clean$BLD)
sf_pums_clean$BLD <- factor(sf_pums_clean$BLD)
sf_pums_clean$TEN < -factor(sf_pums_clean$TEN)
sf_pums_clean$MV < -factor(sf_pums_clean$MV)
sf_pums_clean$PUMA < -factor(sf_pums_clean$PUMA)
logit_model <- glm(
Leadrisk ~ BLD + TEN + MV + PUMA,
family = quasibinomial(),
data = sf_pums_clean
)
summary(logit_model)
sf_pums_clean$BLD <- factor(sf_pums_clean$BLD)
sf_pums_clean$TEN <- factor(sf_pums_clean$TEN)
sf_pums_clean$MV <- factor(sf_pums_clean$MV)
sf_pums_clean$PUMA <- factor(sf_pums_clean$PUMA)
logit_model <- glm(
Leadrisk ~ BLD + TEN + MV + PUMA,
family = quasibinomial(),
data = sf_pums_clean
)
summary(logit_model)
levels(sf_pums_clean$BLD)
sample_n()
?sample_n
sample_n(sf_pums_clean, size = 1)
sf_pums_clean$BLD <- factor(sf_pums_clean$BLD)
sf_pums_clean$TEN <- factor(sf_pums_clean$TEN)
sf_pums_clean$MV <- factor(sf_pums_clean$MV)
sf_pums_clean$PUMA <- factor(sf_pums_clean$PUMA)
logit_model <- glm(
Leadrisk ~ BLD + TEN + MV + PUMA,
family = quasibinomial(),
data = sf_pums_clean
)
summary(logit_model)
X = data.frame(sample_n(sf_pums_clean, size = 1))
predict(logit_model, X, type = "response")
set.seed(2)
X = data.frame(sample_n(sf_pums_clean, size = 1))
predict(logit_model, X, type = "response")
set.seed(5)
X = data.frame(sample_n(sf_pums_clean, size = 1))
predict(logit_model, X, type = "response")
View(X)
set.seed(1)
RandomRow = data.frame(sample_n(sf_pums_clean, size = 1))
predict(logit_model, RandomRow, type = "response")
unique(sf_pums_clean$BLD)
sf_pums_clean$BLD == 1
ONES = subset(sf_pums_clean, sf_pums_clean$BLD == 1)
View(ONES)
bay_county_names <-
c(
"Alameda",
"Contra Costa",
"Marin",
"Napa",
"San Francisco",
"San Mateo",
"Santa Clara",
"Solano",
"Sonoma"
)
bay_counties <-
counties("CA", cb = T, progress_bar = F) %>%
filter(NAME %in% bay_county_names)
bay_pumas <-
ca_pumas %>%
st_centroid() %>%
.[bay_counties, ] %>%
st_drop_geometry() %>%
left_join(ca_pumas %>% select(GEOID10)) %>%
st_as_sf()
bay_pums <-
pums_2019_1yr %>%
mutate(
PUMA = str_pad(public_use_microdata_area,5,"left","0")
) %>%
filter(PUMA %in% bay_pumas$PUMACE10)
View(bay_pums)
pums_2019_1yr <- getCensus(
name = "acs/acs1/pums",
vintage = 2019,
region = "public use microdata area:*",
regionin = "state:06",
vars = c(
"SERIALNO",
"SPORDER",
"PWGTP",
"WGTP",
"AGEP",
"SCHL",
"PINCP",
"RAC1P",
"HISP",
"LANX"
)
)
bay_county_names <-
c(
"Alameda",
"Contra Costa",
"Marin",
"Napa",
"San Francisco",
"San Mateo",
"Santa Clara",
"Solano",
"Sonoma"
)
bay_counties <-
counties("CA", cb = T, progress_bar = F) %>%
filter(NAME %in% bay_county_names)
bay_pumas <-
ca_pumas %>%
st_centroid() %>%
.[bay_counties, ] %>%
st_drop_geometry() %>%
left_join(ca_pumas %>% select(GEOID10)) %>%
st_as_sf()
bay_pums <-
pums_2019_1yr %>%
mutate(
PUMA = str_pad(public_use_microdata_area,5,"left","0")
) %>%
filter(PUMA %in% bay_pumas$PUMACE10)
View(bay_pums)
bay_pums_education <- bay_pums %>%
mutate(
education = SCHL %>%
factor(
levels = bay_pums$SCHL %>%
unique() %>%
as.numeric() %>%
sort()
),
income = as.numeric(PINCP)
)
View(bay_pums_education)
library(mapview)
library(censusapi)
library(tidyverse)
library(tigris)
library(sf)
library(leaflet)
Sys.setenv(CENSUS_KEY="c8aa67e4086b4b5ce3a8717f59faa9a28f611dab")
pums_2019_1yr <- getCensus(
name = "acs/acs1/pums",
vintage = 2019,
region = "public use microdata area:*",
regionin = "state:06",
vars = c(
"SERIALNO",
"SPORDER",
"PWGTP",
"WGTP",
"YBL",
"BLD",
"TEN",
"MV",
"HINCP",
"AGEP"
)
)
# saveRDS(pums_2019_1yr, "a6_pums.rds")
# pums_2019_1yr <- readRDS("a6_pums.rds")
ca_pumas <-
pumas("CA", cb = T, progress_bar = F)
sf_boundary <-
counties("CA", cb = T, progress_bar = F) %>%
filter(NAME == "San Francisco")
sf_pumas <-
ca_pumas %>%
st_centroid() %>%
.[sf_boundary, ] %>%
st_drop_geometry() %>%
left_join(ca_pumas %>% select(GEOID10)) %>%
st_as_sf()
mapview(sf_pumas)
sf_pums <-
pums_2019_1yr %>%
mutate(
PUMA = str_pad(public_use_microdata_area,5,"left","0")
) %>%
filter(PUMA %in% sf_pumas$PUMACE10)
sf_pums_clean <- sf_pums %>% mutate(YBL = as.numeric(YBL), AGEP = as.numeric(AGEP), HINCP = as.numeric(HINCP)) %>%
filter(YBL %in% 1:3) %>%
arrange(AGEP) %>%
group_by(SERIALNO) %>% summarize_all(first) %>% mutate(Leadrisk = ifelse(HINCP < 90000 &
AGEP < 6, 1, 0))
sf_pums_clean <- sf_pums_clean %>% mutate(
BLD = BLD %>%
factor(
levels = sf_pums_clean$BLD %>%
unique() %>%
as.numeric() %>%
sort()
), TEN = TEN %>%
factor(
levels = sf_pums_clean$TEN %>%
unique() %>%
as.numeric() %>%
sort()
), MV = MV %>%
factor(
levels = sf_pums_clean$MV %>%
unique() %>%
as.numeric() %>%
sort()
), PUMA = PUMA %>%
factor(
levels = sf_pums_clean$PUMA %>%
unique() %>%
as.numeric() %>%
sort()
)
)
View(sf_pums_clean)
library(mapview)
library(censusapi)
library(tidyverse)
library(tigris)
library(sf)
library(leaflet)
Sys.setenv(CENSUS_KEY="c8aa67e4086b4b5ce3a8717f59faa9a28f611dab")
pums_2019_1yr <- getCensus(
name = "acs/acs1/pums",
vintage = 2019,
region = "public use microdata area:*",
regionin = "state:06",
vars = c(
"SERIALNO",
"SPORDER",
"PWGTP",
"WGTP",
"YBL",
"BLD",
"TEN",
"MV",
"HINCP",
"AGEP"
)
)
# saveRDS(pums_2019_1yr, "a6_pums.rds")
# pums_2019_1yr <- readRDS("a6_pums.rds")
ca_pumas <-
pumas("CA", cb = T, progress_bar = F)
sf_boundary <-
counties("CA", cb = T, progress_bar = F) %>%
filter(NAME == "San Francisco")
sf_pumas <-
ca_pumas %>%
st_centroid() %>%
.[sf_boundary, ] %>%
st_drop_geometry() %>%
left_join(ca_pumas %>% select(GEOID10)) %>%
st_as_sf()
sf_pums <-
pums_2019_1yr %>%
mutate(
PUMA = str_pad(public_use_microdata_area,5,"left","0")
) %>%
filter(PUMA %in% sf_pumas$PUMACE10)
sf_pums_clean <- sf_pums %>% mutate(YBL = as.numeric(YBL), AGEP = as.numeric(AGEP), HINCP = as.numeric(HINCP)) %>%
filter(YBL %in% 1:3) %>%
arrange(AGEP) %>%
group_by(SERIALNO) %>% summarize_all(first) %>% mutate(Leadrisk = ifelse(HINCP < 90000 &
AGEP < 6, 1, 0))
unique(sf_pums_clean$BLD)
unique(sf_pums_clean$PUMA)
sf_pums_clean <- sf_pums_clean %>% mutate(
BLD = BLD %>%
factor(
levels = sf_pums_clean$BLD %>%
unique() %>%
as.numeric() %>%
sort()
)) %>% mutate(
TEN = TEN %>%
factor(
levels = sf_pums_clean$TEN %>%
unique() %>%
as.numeric() %>%
sort()
)) %>% mutate(MV = MV %>%
factor(
levels = sf_pums_clean$MV %>%
unique() %>%
as.numeric() %>%
sort()
))
unique(sf_pums_clean$BLD)
sf_pums_clean$PUMA <- factor(sf_pums_clean$PUMA)
unique(sf_pums_clean$PUMA)
logit_model <- glm(
Leadrisk ~ BLD + TEN + MV + PUMA,
family = quasibinomial(),
data = sf_pums_clean
)
summary(logit_model)
set.seed(1)
RandomRow = data.frame(sample_n(sf_pums_clean, size = 1))
predict(logit_model, RandomRow, type = "response")
set.seed(1)
RandomRow = data.frame(sample_n(sf_pums_clean, size = 1))
RandomRow
predict(logit_model, RandomRow, type = "response")
set.seed(1)
RandomRow = data.frame(sample_n(sf_pums_clean, size = 1))
print(RandomRow)
predict(logit_model, RandomRow, type = "response")
Scores = predict(logit_model, data.frame(sf_pums_clean), type = "response")
Scores
Updated_Pums = cbind(sf_pums_clean, Scores)
View(Updated_Pums)
Scores = predict(logit_model, data.frame(sf_pums_clean), type = "response")
PredictedRisk = ifelse(Scores >= 0.10, 1, 0)
PredictedRisk
sum(PredictedRisk == 1)
Scores = predict(logit_model, data.frame(sf_pums_clean), type = "response")
PredictedRisk = ifelse(Scores >= 0.10, 1, 0)
Updated_Pums = cbind(sf_pums_clean, PredictedRisk)
View(sf_pums_clean)
Scores = predict(logit_model, data.frame(sf_pums_clean), type = "response")
PredictedRisk = ifelse(Scores >= 0.10, 1, 0)
summary_2x2 <-
sf_pums_clean %>%
mutate(
Leadrisk = ifelse(
Leadrisk == 1,
"Risk",
"No Risk"
)
) %>%
pull(Leadrisk) %>%
table(Scores > 0.10)
summary_2x2
temp <- tempfile()
download.file("https://www2.census.gov/programs-surveys/acs/data/pums/2019/1-Year/csv_hca.zip",destfile = temp)
pums_hca_2019_1yr <- read_csv(unzip(temp,"psam_h06.csv"))
unlink(temp)
temp
View(pums_2019_1yr)
Overall = pums_hca_2019_1yr %>% left_join(sf_pums_clean, by = "SERIALNO")
View(Overall)
View(Overall)
1
